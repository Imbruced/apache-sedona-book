{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd627bb-7886-4970-ace7-d99a50da0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sedona.spark import SedonaContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66077ae6-71a3-4cb0-ba25-5d7039f9d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 19:42:04 WARN Utils: Your hostname, Pawels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.115 instead (on interface en0)\n",
      "24/12/18 19:42:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pawelkocinski/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pawelkocinski/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pawelkocinski/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      "mysql#mysql-connector-java added as a dependency\n",
      "com.oracle.database.jdbc#ojdbc8 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client-api added as a dependency\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "uk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0e4aca9e-f493-4699-bbaf-6d82b29db881;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.4 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.oracle.database.jdbc#ojdbc8;23.6.0.24.10 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.6 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.367 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.4 in local-m2-cache\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.7.0 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.7.0 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.20.0 in local-m2-cache\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.7.0 in central\n",
      "\tfound org.xerial#sqlite-jdbc;3.41.2.2 in local-m2-cache\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in local-m2-cache\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.7.0-28.5 in central\n",
      "\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n",
      "\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n",
      ":: resolution report :: resolve 3871ms :: artifacts dl 49ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tcom.oracle.database.jdbc#ojdbc8;23.6.0.24.10 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from local-m2-cache in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.6 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.7.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.7.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.7.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.7.0-28.5 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.20.0 from local-m2-cache in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from local-m2-cache in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.4 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from local-m2-cache in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\torg.xerial#sqlite-jdbc;3.41.2.2 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.4 from local-m2-cache in [default]\n",
      "\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 by [org.xerial.snappy#snappy-java;1.1.10.4] in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 by [org.apache.hadoop#hadoop-client-api;3.4.1] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.42.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 by [org.checkerframework#checker-qual;3.42.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   55  |   1   |   0   |   7   ||   47  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0e4aca9e-f493-4699-bbaf-6d82b29db881\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 47 already retrieved (0kB/36ms)\n",
      "24/12/18 19:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-12-18 19:42:20.095 java[22532:14702269] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-18 19:42:20.096 java[22532:14702269] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "additional_packages = [\n",
    "    'org.postgresql:postgresql:42.7.4',\n",
    "    'mysql:mysql-connector-java:8.0.33',\n",
    "    'com.oracle.database.jdbc:ojdbc8:23.6.0.24.10',\n",
    "    'org.mongodb.spark:mongo-spark-connector_2.12:10.4.0',\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1',\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.6',\n",
    "    'org.apache.hadoop:hadoop-client-api:3.4.1',\n",
    "    'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0',\n",
    "    'org.datasyslab:geotools-wrapper:1.7.0-28.5',\n",
    "    'uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4'\n",
    "]\n",
    "\n",
    "config = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\n",
    "     \"spark.driver.memory\", \"2G\"   \n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"sedona\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"sedona_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\n",
    "      \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "      \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(additional_packages))\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d1336-990f-4586-9d5d-13ce64ec39c4",
   "metadata": {},
   "source": [
    "# Postgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01064422-bf4d-4a1d-a566-024398c42fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   name|            location|\n",
      "+-------+--------------------+\n",
      "|Point A|0101000020E610000...|\n",
      "|Point B|0101000020E610000...|\n",
      "|Point C|0101000020E610000...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "database_name = \"sedona\"\n",
    "user_name = \"sedona\"\n",
    "password = \"sedona\"\n",
    "\n",
    "postgresql_url = f\"jdbc:postgresql://localhost:5432/{database_name}\"\n",
    "postgresql_properties = {\n",
    "    \"user\": \"sedona\",\n",
    "    \"password\": \"postgis\",\n",
    "}\n",
    "\n",
    "table_name = \"points\"\n",
    "df = sedona.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgresql_url) \\\n",
    "    .option(\"user\", postgresql_properties[\"user\"]) \\\n",
    "    .option(\"password\", postgresql_properties[\"password\"]) \\\n",
    "    .option(\"dbtable\", \"points\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6433eff-52a3-45ec-9348-b6fcc6aeb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming postgis EWKB column to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c77780-4f80-4bd6-9acd-5ec5e01724a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   name|         geom|\n",
      "+-------+-------------+\n",
      "|Point A|POINT (10 20)|\n",
      "|Point B|POINT (30 40)|\n",
      "|Point C|POINT (50 60)|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"name\", \"ST_GeomFromEWKB(location) AS geom\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11acaa5d-cf55-49f7-a9cc-14b838dbb3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name|geom|\n",
      "+-------+----+\n",
      "|Point A|4326|\n",
      "|Point B|4326|\n",
      "|Point C|4326|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"name\", \"ST_SRID(ST_GeomFromEWKB(location)) AS geom\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469304c-758d-4242-926d-83765ddbacad",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91a3931-393c-4ea1-b7c2-304a5a790995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   name|            location|\n",
      "+-------+--------------------+\n",
      "|Point A|[E6 10 00 00 01 0...|\n",
      "|Point B|[E6 10 00 00 01 0...|\n",
      "|Point C|[E6 10 00 00 01 0...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "database_name = \"sedona\"\n",
    "user_name = \"sedona\"\n",
    "password = \"sedona\"\n",
    "table_name = \"points\"\n",
    "\n",
    "mysql_url = f\"jdbc:mysql://localhost:3306/{database_name}\"\n",
    "\n",
    "df = sedona.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", mysql_url) \\\n",
    "    .option(\"user\", user_name) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", \"points\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e1d56c-6af4-4e5c-91ae-3fa75cd64039",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `ST_GeomFromMySQL` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mST_GeomFromMySQL(location) AS geom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3269\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   3268\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 3269\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `ST_GeomFromMySQL` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 0"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"name\",\n",
    "    \"ST_GeomFromMySQL(location) AS geom\"\n",
    ").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d6a5f-a057-4be7-98f1-9bf9aa2106f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\n",
    "    \"name\",\n",
    "    \"ST_SRID(ST_GeomFromMySQL(location)) AS srid\"\n",
    ").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e1790-4ccc-4cce-8419-f5a9e0b3e5b8",
   "metadata": {},
   "source": [
    "# OracleDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b62843-8430-4708-8e3d-3237a40cc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_url = \"jdbc:oracle:thin:@//localhost:1521/FREEPDB1\"\n",
    "\n",
    "table_name = \"points\"\n",
    "\n",
    "df = sedona.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", oracle_url) \\\n",
    "    .option(\"query\", \"SELECT name, SDO_UTIL.TO_WKBGEOMETRY(location) AS wkb from points\") \\\n",
    "    .option(\"user\", \"sedona\") \\\n",
    "    .option(\"password\", \"sedona\") \\\n",
    "    .option(\"driver\", \"oracle.jdbc.OracleDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce293d13-25b2-46ab-a01f-b22627654f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   NAME|                 WKB|\n",
      "+-------+--------------------+\n",
      "|Point A|[00 00 00 00 01 4...|\n",
      "|Point B|[00 00 00 00 01 4...|\n",
      "|Point C|[00 00 00 00 01 4...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42401db-d2d7-4a00-8557-e91454f50f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         geom|\n",
      "+-------------+\n",
      "|POINT (10 20)|\n",
      "|POINT (30 40)|\n",
      "|POINT (50 60)|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform Data Processing\n",
    "df.selectExpr(\"ST_GeomFromWKB(wkb) as geom\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f67465-df5c-4134-af8b-1e7f14608446",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc09c7a0-977a-47b1-81a0-c069b5f37ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|location                     |\n",
      "+-----------------------------+\n",
      "|{Point, [-74.006, 40.7128]}  |\n",
      "|{Point, [-118.2437, 34.0522]}|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_json\n",
    "\n",
    "df = sedona.read \\\n",
    "    .option(\"database\", \"sedona\") \\\n",
    "    .option(\"collection\", \"points\") \\\n",
    "    .format(\"mongodb\").load()\n",
    "\n",
    "df.selectExpr(\"location\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b0ab46-0c5d-4947-8001-ede3d5d4c8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7e2975-5ce7-427c-83ce-55504bd68f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+\n",
      "|name   |geom                     |\n",
      "+-------+-------------------------+\n",
      "|Point A|POINT (-74.006 40.7128)  |\n",
      "|Point B|POINT (-118.2437 34.0522)|\n",
      "+-------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"location\", to_json(col(\"location\")))\\\n",
    "    .selectExpr(\"name\", \"ST_GeomFromGeoJSON(location) AS geom\")\\\n",
    "    .show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a180c6c-0366-41a7-94c5-1bd690bf76dd",
   "metadata": {},
   "source": [
    "# Bulk read from postgres and store as geoparquet on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c34f64-ee5f-47cf-a65d-03b9101c79fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-------------------+-------------------+\n",
      "|     name|st_geomfromewkb(location)|         created_at|         updated_at|\n",
      "+---------+-------------------------+-------------------+-------------------+\n",
      "|Polygon B|     POLYGON ((70 80, ...|2020-01-01 00:00:00|2024-01-01 11:36:00|\n",
      "|Polygon C|     POLYGON ((130 140...|2023-01-01 00:00:00|2024-01-01 11:16:00|\n",
      "+---------+-------------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "database_name = \"sedona\"\n",
    "user_name = \"sedona\"\n",
    "password = \"sedona\"\n",
    "\n",
    "postgresql_url = f\"jdbc:postgresql://localhost:5432/{database_name}\"\n",
    "postgresql_properties = {\n",
    "    \"user\": \"sedona\",\n",
    "    \"password\": \"postgis\",\n",
    "}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM polygons WHERE COALESCE(CREATED_AT, UPDATED_AT) BETWEEN '2020-01-01 00:00:00' AND '2024-01-01 00:00:00'\n",
    "\"\"\"\n",
    "\n",
    "start_date = \"2024-01-01 11:00:00\"\n",
    "end_date = \"2024-01-01 12:00:00\"\n",
    "table_name = \"points\"\n",
    "df = sedona.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgresql_url) \\\n",
    "    .option(\"user\", postgresql_properties[\"user\"]) \\\n",
    "    .option(\"password\", postgresql_properties[\"password\"]) \\\n",
    "    .option(\"query\", query) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.selectExpr(\"name\", \"ST_GeomFromEWKB(location)\", \"created_at\", \"updated_at\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e978a-e72e-4b28-8808-843903d3f6b5",
   "metadata": {},
   "source": [
    "# Reading and storing cdc data into s3 geoparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8001c1-cbac-4352-aca1-1c52ac243446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t \n",
    "\n",
    "df = sedona \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"sedona-debezium.public.points\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c88ca1-c8a7-4578-8121-c9af80055b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = t.StructType([\n",
    "    t.StructField(\"name\", t.StringType(), True),\n",
    "    t.StructField(\"location\", t.StructType([\n",
    "        t.StructField(\"srid\", t.IntegerType()),\n",
    "        t.StructField(\"wkb\", t.BinaryType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "before = t.StructField(\"before\", data, True)\n",
    "\n",
    "after = t.StructField(\"after\", data, True)\n",
    "\n",
    "payload = t.StructField(\n",
    "    \"payload\", t.StructType([before, after])\n",
    ")\n",
    "\n",
    "schema = t.StructType([\n",
    "    payload\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cdf4db5-7bc6-48cf-9ed7-b2db7342c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_df = df.select(f.from_json(f.expr(\"CAST(value AS STRING)\"), schema).alias(\"data\"))\\\n",
    "    .selectExpr(\n",
    "        \"data.payload.after.name as name\",\n",
    "        \"data.payload.after.location.wkb as wkb\",\n",
    "        \"data.payload.after.location.srid AS srid\"\n",
    "    )\\\n",
    "    .selectExpr(\"name\", \"ST_SetSRID(ST_GeomFromWKB(wkb), srid) AS geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0718ae64-a1d6-43e7-ac74-93ceaac3d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 19:42:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/12/18 19:42:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "geometry_df.withColumn(\"geohash\", f.expr(\"ST_GeoHash(geom, 5)\"))\\\n",
    "    .orderBy(\"geohash\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"geoparquet\")\\\n",
    "    .save(\"s3a://sedona/postgis-cdc-batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af31a0-dc7d-4115-9496-524fe080792f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
