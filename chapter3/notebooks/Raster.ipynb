{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18385ec-e678-44cc-819f-eaba6a0aa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sedona.raster_utils import SedonaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f8e863-b601-4d14-bb61-ff382f4d347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 19:39:05 WARN Utils: Your hostname, Pawels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.115 instead (on interface en0)\n",
      "24/12/18 19:39:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/pawelkocinski/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/pawelkocinski/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/pawelkocinski/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client-api added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "uk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-84dcb853-d67f-4297-aa6e-7755ccdee824;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in local-m2-cache\n",
      "\tfound com.google.guava#guava;27.0-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in local-m2-cache\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in local-m2-cache\n",
      "\tfound commons-cli#commons-cli;1.2 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound commons-io#commons-io;2.8.0 in local-m2-cache\n",
      "\tfound commons-net#commons-net;3.6 in local-m2-cache\n",
      "\tfound commons-collections#commons-collections;3.2.2 in local-m2-cache\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in local-m2-cache\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in local-m2-cache\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in local-m2-cache\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in local-m2-cache\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in local-m2-cache\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-text;1.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in local-m2-cache\n",
      "\tfound org.apache.avro#avro;1.7.7 in local-m2-cache\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-compress;1.21 in local-m2-cache\n",
      "\tfound com.google.re2j#re2j;1.1 in local-m2-cache\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in local-m2-cache\n",
      "\tfound com.google.code.gson#gson;2.8.9 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in local-m2-cache\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in local-m2-cache\n",
      "\tfound net.minidev#json-smart;2.4.7 in local-m2-cache\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in local-m2-cache\n",
      "\tfound org.ow2.asm#asm;5.0.4 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in local-m2-cache\n",
      "\tfound com.jcraft#jsch;0.1.55 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in local-m2-cache\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in local-m2-cache\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in local-m2-cache\n",
      "\tfound dnsjava#dnsjava;2.1.7 in local-m2-cache\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in local-m2-cache\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in local-m2-cache\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.7.0 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.7.0 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.20.0 in local-m2-cache\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.7.0 in central\n",
      "\tfound org.xerial#sqlite-jdbc;3.41.2.2 in local-m2-cache\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in local-m2-cache\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.7.0-28.5 in central\n",
      "\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n",
      "\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n",
      ":: resolution report :: resolve 3044ms :: artifacts dl 154ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from local-m2-cache in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from local-m2-cache in [default]\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from local-m2-cache in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from local-m2-cache in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from local-m2-cache in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from local-m2-cache in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from local-m2-cache in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from local-m2-cache in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from local-m2-cache in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from local-m2-cache in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from local-m2-cache in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from local-m2-cache in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tcommons-net#commons-net;3.6 from local-m2-cache in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from local-m2-cache in [default]\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from local-m2-cache in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from local-m2-cache in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from local-m2-cache in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from local-m2-cache in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from local-m2-cache in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from local-m2-cache in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from local-m2-cache in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from local-m2-cache in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.sedona#sedona-common;1.7.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.7.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.7.0 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from local-m2-cache in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from local-m2-cache in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.7.0-28.5 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.locationtech.jts#jts-core;1.20.0 from local-m2-cache in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from local-m2-cache in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from local-m2-cache in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\torg.xerial#sqlite-jdbc;3.41.2.2 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from local-m2-cache in [default]\n",
      "\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.5.2 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 by [org.apache.commons#commons-math3;3.6.1] in [default]\n",
      "\tcom.google.guava#guava;25.1-jre by [com.google.guava#guava;27.0-jre] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  118  |   0   |   0   |   3   ||  115  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-84dcb853-d67f-4297-aa6e-7755ccdee824\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 115 already retrieved (0kB/92ms)\n",
      "24/12/18 19:39:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-12-18 19:39:24.672 java[21980:14688783] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-18 19:39:24.673 java[21980:14688783] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "additional_packages = [\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-client-api:3.3.4',\n",
    "    'org.apache.hadoop:hadoop-common:3.3.4',\n",
    "    'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0',\n",
    "    'org.datasyslab:geotools-wrapper:1.7.0-28.5',\n",
    "    'uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4'\n",
    "]\n",
    "\n",
    "config = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\"spark.jars.packages\", \",\".join(additional_packages))\\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"sedona\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"sedona_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\n",
    "      \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "      \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "\n",
    "sc = sedona.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd18decf-4785-4f2c-9753-b86a4ec70f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 19:39:27 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "geotiff_df = sedona\\\n",
    "    .read\\\n",
    "    .format(\"binaryFile\")\\\n",
    "    .load(\"s3a://sedona/raster/japan_landsat_part.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43f8979-fe53-45fe-8322-b8108b8d340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_df\\\n",
    "    .selectExpr(\"RS_FromGeoTiff(content) AS raster\")\\\n",
    "    .createOrReplaceTempView(\"japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fdd126d-6ca9-43c4-9461-930208398afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona\\\n",
    "    .sql(\"SELECT EXPLODE(RS_Tile(raster, 10, 10)) AS tile FROM japan\")\\\n",
    "    .createOrReplaceTempView(\"tiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77c89b9-7edd-4d61-985d-63033e28e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wkt = 'POLYGON ((747726.6770902255084366 3856119.2088216841220856, 741397.5797568620182574 3853730.8702053208835423, 734590.8147002257173881 3856238.6257525025866926, 729455.8866750439628959 3861373.5537776844576001, 732321.8930146803613752 3866030.8140795934014022, 735784.9840084075694904 3868657.9865575931034982, 733754.8961844984441996 3871046.3251739568077028, 729336.4697442258475348 3869374.4881425024941564, 723365.6232033168198541 3864836.6447714115493000, 718350.1121089532971382 3861731.8045701389200985, 713692.8518070442369208 3859343.4659537752158940, 708796.7576434988295659 3862925.9738783207722008, 705572.5005114079685882 3869971.5727965934202075, 707244.3375428625149652 3876778.3378532296046615, 710468.5946749533759430 3885854.0245954110287130, 716678.2750774987507612 3892899.6235136836767197, 723007.3724108622409403 3894571.4605451384559274, 731127.7237064985092729 3896362.7145074112340808, 739725.7427254074718803 3896243.2975765927694738, 747129.5924361345823854 3894690.8774759564548731, 752264.5204613163368776 3892421.9557904112152755, 753936.3574927708832547 3889914.2002432295121253, 755011.1098701345035806 3886451.1092495019547641, 757638.2823481344385073 3885256.9399413201026618, 759310.1193795889848843 3882510.3505325019359589, 759310.1193795889848843 3879644.3441928657703102, 758354.7839330435963348 3878330.7579538659192622, 757757.6992789526702836 3876539.5039915931411088, 757877.1162097707856447 3874628.8330985023640096, 757996.5331405890174210 3872837.5791362295858562, 758593.6177946799434721 3871165.7421047748066485, 756682.9469015890499577 3870449.2405198658816516, 756563.5299707708181813 3868299.7357651386409998, 755966.4453166800085455 3864597.8109097750857472, 755369.3606625890824944 3861731.8045701389200985, 755369.3606625890824944 3859701.7167462296783924, 751309.1850147709483281 3857791.0458531389012933, 747726.6770902255084366 3856119.2088216841220856))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57576e3f-2399-44a2-b748-7913dee1d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\n",
    "    f\"\"\"\n",
    "    WITH algebra_result AS (\n",
    "        SELECT \n",
    "            RS_MapAlgebra(tile, 'D', 'out = (rast[1] - rast[0]) / (rast[1] + rast[0]);') AS raster,\n",
    "            ST_SetSRID(ST_GeomFromText('{wkt}'), 32653) AS geom\n",
    "        FROM tiles\n",
    "    ),\n",
    "    clipped AS (\n",
    "        SELECT RS_Clip(raster, 1, geom, 0, false) AS raster\n",
    "        FROM algebra_result\n",
    "        WHERE RS_Intersects(geom, raster)\n",
    "    )\n",
    "    SELECT raster\n",
    "    FROM clipped\n",
    "    \"\"\"\n",
    ").createOrReplaceTempView(\"clipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b5a852-f239-4322-9f33-363ae22a036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.7.1 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.7.1 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.7.1 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.7.1 used for parser compilation does not match the current runtime version 4.9.3\n",
      "24/12/18 19:40:14 WARN VectorToRasterProcess: coercing double feature values to float raster values\n",
      "24/12/18 19:40:14 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 2)\n",
      "org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n",
      "│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n",
      "│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n",
      "└ Image=WritableRenderedImageAdapter[]\n",
      ", 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: 100\n",
      "\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n",
      "\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n",
      "\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n",
      "\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n",
      "\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n",
      "\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\t... 20 more\n",
      "24/12/18 19:40:14 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 2) (192.168.0.115 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n",
      "│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n",
      "│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n",
      "└ Image=WritableRenderedImageAdapter[]\n",
      ", 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException: 100\n",
      "\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n",
      "\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n",
      "\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n",
      "\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n",
      "\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n",
      "\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n",
      "\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n",
      "\t... 20 more\n",
      "\n",
      "24/12/18 19:40:14 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o71.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (192.168.0.115 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n└ Image=WritableRenderedImageAdapter[]\n, 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 100\n\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n└ Image=WritableRenderedImageAdapter[]\n, 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 100\n\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msedona\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect * from clipped\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Desktop/projects/apache-sedona-book/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o71.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (192.168.0.115 executor driver): org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n└ Image=WritableRenderedImageAdapter[]\n, 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 100\n\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.sedona_sql.expressions.InferredExpressionException: Exception occurred while evaluating expression RS_Clip - inputs: [GridCoverage2D[\"genericCoverage\", GeneralEnvelope[(729960.0, 3896100.0), (730260.0, 3896400.0)], DefaultProjectedCRS[\"WGS 84 / UTM zone 53N\"]]\n│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n│     ‣ Category(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\n└ Image=WritableRenderedImageAdapter[]\n, 1, POLYGON ((747726.6770902255 3856119.208821684, 741397.579756862 3853730.870205321, 734590.8147002257 3856238.6257525026, 729455.886675044 3861373.5537776845, 732321.8930146804 3866030.8140795934, 735784.9840084076 3868657.986557593, 733754.8961844984 3871046.325173957, 729336.4697442258 3869374.4881425025, 723365.6232033168 3864836.6447714115, 718350.1121089533 3861731.804570139, 713692.8518070442 3859343.465953775, 708796.7576434988 3862925.973878321, 705572.500511408 3869971.5727965934, 707244.3375428625 3876778.3378532296, 710468.5946749534 3885854.024595411, 716678.2750774988 3892899.6235136837, 723007.3724108622 3894571.4605451385, 731127.7237064985 3896362.7145074112, 739725.7427254075 3896243.297576593, 747129.5924361346 3894690.8774759565, 752264.5204613163 3892421.955790411, 753936.3574927709 3889914.2002432295, 755011.1098701345 3886451.109249502, 757638.2823481344 3885256.93994132, 759310.119379589 3882510.350532502, 759310.119379589 3879644.344192866, 758354.7839330436 3878330.757953866, 757757.6992789527 3876539.503991593, 757877.1162097708 3874628.8330985024, 757996.533140589 3872837.5791362296, 758593.61779468 3871165.742104775, 756682.946901589 3870449.240519866, 756563.5299707708 3868299.7357651386, 755966.44531668 3864597.810909775, 755369.3606625891 3861731.804570139, 755369.3606625891 3859701.7167462297, 751309.185014771 3857791.045853139, 747726.6770902255 3856119.208821684)), 0.0, false], cause: 100\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression$.throwExpressionInferenceException(InferredExpression.scala:149)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:113)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:389)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 100\n\tat java.awt.image.DataBufferDouble.getElemDouble(DataBufferDouble.java:372)\n\tat javax.media.jai.ComponentSampleModelJAI.getSampleDouble(ComponentSampleModelJAI.java:987)\n\tat java.awt.image.SampleModel.getPixel(SampleModel.java:743)\n\tat java.awt.image.Raster.getPixel(Raster.java:1558)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:368)\n\tat org.apache.sedona.common.raster.RasterBandEditors.clip(RasterBandEditors.java:416)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.raster.RS_Clip$$anonfun$$lessinit$greater$14.apply(RasterBandEditors.scala:63)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferrableFunctionConverter$.$anonfun$inferrableFunction5$2(InferrableFunctionConverter.scala:139)\n\tat org.apache.spark.sql.sedona_sql.expressions.InferredExpression.eval(InferredExpression.scala:107)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# here is the error that need to be addresses in Sedona\n",
    "sedona.sql(\"select * from clipped\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7d54b-5294-43ce-880a-cb4aa0a0c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
